{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10_scraping_earthquake_data.ipynb\n",
    "<p style=\"background-color:#fff6e4; padding:15px; border-width:3px; border-color:#f5ecda; border-style:solid; border-radius:6px\"><b>This script scrapes the earthquake data from the USGS database.</b> Main parts of the routines have been developed in previous courses at the University of London by the same author (Mohr, 2021, 2023, 2024a) and have been developed further to fulfil the needs of the scraping procedure for this MSc thesis. However, the code has been modified to fulfil the latest requirements and package inter-dependencies. Some comments will be added in this Jupyter Notebook and the code has several inline comments. For the project/research itself, see the appropriate document.\n",
    "</p>\n",
    "\n",
    "#### References (for this script)\n",
    "*Mohr, S. (2021) Regional Spatial Clusters of Earthquakes at the Pacific Ring of Fire: Analysing Data from the USGS ANSS ComCat and Building Regional Spatial Clusters. DSM020, Python, examined coursework cw1. University of London.*\n",
    "\n",
    "*Mohr, S. (2023) Clustering of Earthquakes on a Worldwide Scale with the Help of Big Data Machine Learning Methods. DSM010, Big Data, examined coursework cw2. University of London.*\n",
    "\n",
    "*Mohr, S. (2024a) Comparing Different Tectonic Setups Considering Publicly Available Basic Earthquakeâ€™s Data. DSM050, Data Visualisation, examined coursework cw1. University of London.*\n",
    "\n",
    "#### History\n",
    "<pre>\n",
    "241016 Generation from previous courseworks at the UoL, setup basic logging, improve error handling for saving the data\n",
    "241017 Reformatting names and variables, use library os for path creation, use also time for filename,\n",
    "       rewrite and add prcoceduresave_dataset, handle verbosity in the same manner, save_dataset: docstring, \n",
    "       add logging and docstring to get_data_from_usgs_anss_api, function calling with explicit var names\n",
    "241018 Add errorhandling & logging, docstring to query_earthquakes, export shared procedures to shared_procedures.py and \n",
    "       import them from there as a general solution for sharing identical procedures between scripts for this project,\n",
    "       re-write query_earthquakes to use get_data_from_web_api, move get_data_from_web_api to shared_procedures.py\n",
    "241203 Repair query by constructing a combined dictionary with times and params, set parameters and scrape data\n",
    "250104 Check docstrings\n",
    "250110 Set Lon=[100;180][-180;-60] and Lat=[-70;70] because of buffer around plate margins of the Pacific-,\n",
    "       Philippine-, Cocos-, and Nazca-Plate\n",
    "</pre>\n",
    "\n",
    "#### Todo\n",
    "<pre>./.</pre>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparing the environment\n",
    "### System information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "******* Python Info ***********\n",
      "/bin/python\n",
      "Python 3.8.10\n",
      "\n",
      "******* CPU Info ***********\n",
      "Architecture:                       x86_64\n",
      "CPU op-mode(s):                     32-bit, 64-bit\n",
      "Byte Order:                         Little Endian\n",
      "Address sizes:                      46 bits physical, 48 bits virtual\n",
      "CPU(s):                             64\n",
      "On-line CPU(s) list:                0-63\n",
      "Thread(s) per core:                 2\n",
      "Core(s) per socket:                 8\n",
      "Socket(s):                          4\n",
      "NUMA node(s):                       4\n",
      "Vendor ID:                          GenuineIntel\n",
      "CPU family:                         6\n",
      "Model:                              85\n",
      "Model name:                         Intel(R) Xeon(R) Gold 6234 CPU @ 3.30GHz\n",
      "Stepping:                           7\n",
      "CPU MHz:                            1200.047\n",
      "CPU max MHz:                        4000.0000\n",
      "CPU min MHz:                        1200.0000\n",
      "BogoMIPS:                           6600.00\n",
      "Virtualization:                     VT-x\n",
      "L1d cache:                          1 MiB\n",
      "L1i cache:                          1 MiB\n",
      "L2 cache:                           32 MiB\n",
      "L3 cache:                           99 MiB\n",
      "NUMA node0 CPU(s):                  0,4,8,12,16,20,24,28,32,36,40,44,48,52,56,60\n",
      "NUMA node1 CPU(s):                  1,5,9,13,17,21,25,29,33,37,41,45,49,53,57,61\n",
      "NUMA node2 CPU(s):                  2,6,10,14,18,22,26,30,34,38,42,46,50,54,58,6\n",
      "                                    2\n",
      "NUMA node3 CPU(s):                  3,7,11,15,19,23,27,31,35,39,43,47,51,55,59,6\n",
      "                                    3\n",
      "Vulnerability Gather data sampling: Mitigation; Microcode\n",
      "Vulnerability Itlb multihit:        KVM: Mitigation: Split huge pages\n",
      "Vulnerability L1tf:                 Not affected\n",
      "Vulnerability Mds:                  Not affected\n",
      "Vulnerability Meltdown:             Not affected\n",
      "Vulnerability Mmio stale data:      Mitigation; Clear CPU buffers; SMT vulnerabl\n",
      "                                    e\n",
      "Vulnerability Retbleed:             Mitigation; Enhanced IBRS\n",
      "Vulnerability Spec store bypass:    Mitigation; Speculative Store Bypass disable\n",
      "                                    d via prctl and seccomp\n",
      "Vulnerability Spectre v1:           Mitigation; usercopy/swapgs barriers and __u\n",
      "                                    ser pointer sanitization\n",
      "Vulnerability Spectre v2:           Mitigation; Enhanced IBRS; IBPB conditional;\n",
      "                                     RSB filling; PBRSB-eIBRS SW sequence; BHI V\n",
      "                                    ulnerable, KVM SW loop\n",
      "Vulnerability Srbds:                Not affected\n",
      "Vulnerability Tsx async abort:      Mitigation; TSX disabled\n",
      "Flags:                              fpu vme de pse tsc msr pae mce cx8 apic sep \n",
      "                                    mtrr pge mca cmov pat pse36 clflush dts acpi\n",
      "                                     mmx fxsr sse sse2 ss ht tm pbe syscall nx p\n",
      "                                    dpe1gb rdtscp lm constant_tsc art arch_perfm\n",
      "                                    on pebs bts rep_good nopl xtopology nonstop_\n",
      "                                    tsc cpuid aperfmperf pni pclmulqdq dtes64 mo\n",
      "                                    nitor ds_cpl vmx smx est tm2 ssse3 sdbg fma \n",
      "                                    cx16 xtpr pdcm pcid dca sse4_1 sse4_2 x2apic\n",
      "                                     movbe popcnt tsc_deadline_timer aes xsave a\n",
      "                                    vx f16c rdrand lahf_lm abm 3dnowprefetch cpu\n",
      "                                    id_fault epb cat_l3 cdp_l3 invpcid_single in\n",
      "                                    tel_ppin ssbd mba ibrs ibpb stibp ibrs_enhan\n",
      "                                    ced tpr_shadow vnmi flexpriority ept vpid ep\n",
      "                                    t_ad fsgsbase tsc_adjust bmi1 avx2 smep bmi2\n",
      "                                     erms invpcid cqm mpx rdt_a avx512f avx512dq\n",
      "                                     rdseed adx smap clflushopt clwb intel_pt av\n",
      "                                    x512cd avx512bw avx512vl xsaveopt xsavec xge\n",
      "                                    tbv1 xsaves cqm_llc cqm_occup_llc cqm_mbm_to\n",
      "                                    tal cqm_mbm_local dtherm ida arat pln pts hw\n",
      "                                    p hwp_act_window hwp_epp hwp_pkg_req pku osp\n",
      "                                    ke avx512_vnni md_clear flush_l1d arch_capab\n",
      "                                    ilities\n",
      "\n",
      "******* RAM Info (in GB) ***********\n",
      "              total        used        free      shared  buff/cache   available\n",
      "Mem:            251          58         124           0          68         191\n",
      "Swap:            63          23          40\n"
     ]
    }
   ],
   "source": [
    "# which python installation and version are we using here?\n",
    "print('\\n******* Python Info ***********')\n",
    "!which python\n",
    "!python --version\n",
    "\n",
    "# show some CPU and RAM info\n",
    "print('\\n******* CPU Info ***********')\n",
    "!lscpu\n",
    "print('\\n******* RAM Info (in GB) ***********')\n",
    "!free -g"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "absl-py==2.1.0\r\n",
      "affine==2.4.0\r\n",
      "aggdraw==1.3.16\r\n",
      "array-record==0.4.0\r\n",
      "asttokens==2.4.1\r\n",
      "astunparse==1.6.3\r\n",
      "atomicwrites==1.1.5\r\n",
      "attrs==19.3.0\r\n",
      "Automat==0.8.0\r\n",
      "backcall==0.2.0\r\n",
      "beautifulsoup4==4.8.2\r\n",
      "blinker==1.4\r\n",
      "cachetools==5.5.0\r\n",
      "certifi==2019.11.28\r\n",
      "chardet==3.0.4\r\n",
      "click==8.1.7\r\n",
      "click-plugins==1.1.1\r\n",
      "cligj==0.7.2\r\n",
      "cloud-init==24.3.1\r\n",
      "colorama==0.4.3\r\n",
      "comm==0.2.2\r\n",
      "command-not-found==0.3\r\n",
      "configobj==5.0.6\r\n",
      "confluent-kafka==2.5.3\r\n",
      "constantly==15.1.0\r\n",
      "contextily==1.5.2\r\n",
      "contourpy==1.1.1\r\n",
      "cryptography==2.8\r\n",
      "cupshelpers==1.0\r\n",
      "cycler==0.10.0\r\n",
      "dbus-python==1.2.16\r\n",
      "debugpy==1.8.7\r\n",
      "decorator==4.4.2\r\n",
      "defer==1.0.6\r\n",
      "distro==1.4.0\r\n",
      "distro-info==0.23+ubuntu1.1\r\n",
      "dm-tree==0.1.8\r\n",
      "entrypoints==0.3\r\n",
      "et-xmlfile==1.0.1\r\n",
      "etils==1.3.0\r\n",
      "executing==2.0.1\r\n",
      "fail2ban==0.11.1\r\n",
      "fastjsonschema==2.20.0\r\n",
      "filelock==3.13.1\r\n",
      "fiona==1.9.6\r\n",
      "flatbuffers==24.3.25\r\n",
      "fonttools==4.53.1\r\n",
      "fsspec==2023.12.2\r\n",
      "ftfy==6.2.0\r\n",
      "gast==0.4.0\r\n",
      "geographiclib==2.0\r\n",
      "geopandas==0.13.2\r\n",
      "geopy==2.4.1\r\n",
      "google-auth==2.36.0\r\n",
      "google-auth-oauthlib==1.0.0\r\n",
      "google-pasta==0.2.0\r\n",
      "googleapis-common-protos==1.66.0\r\n",
      "graphviz==0.20.3\r\n",
      "grpcio==1.68.1\r\n",
      "h5py==3.11.0\r\n",
      "hdbscan==0.8.40\r\n",
      "html5lib==1.0.1\r\n",
      "httplib2==0.14.0\r\n",
      "huggingface-hub==0.22.2\r\n",
      "hyperlink==19.0.0\r\n",
      "idna==2.8\r\n",
      "import-ipynb==0.2\r\n",
      "importlib-metadata==8.5.0\r\n",
      "importlib-resources==6.4.0\r\n",
      "incremental==16.10.1\r\n",
      "ipykernel==6.29.5\r\n",
      "ipynb==0.5.1\r\n",
      "ipynbname==2024.1.0.0\r\n",
      "ipython==8.12.3\r\n",
      "jdcal==1.0\r\n",
      "jedi==0.19.1\r\n",
      "Jinja2==2.10.1\r\n",
      "joblib==1.4.2\r\n",
      "jsonpatch==1.22\r\n",
      "jsonpointer==2.0\r\n",
      "jsonschema==3.2.0\r\n",
      "jupyter-client==8.6.3\r\n",
      "jupyter-core==5.7.2\r\n",
      "keras==2.13.1\r\n",
      "keyring==18.0.1\r\n",
      "kiwisolver==1.3.1\r\n",
      "language-selector==0.1\r\n",
      "launchpadlib==1.10.13\r\n",
      "lazr.restfulclient==0.14.2\r\n",
      "lazr.uri==1.0.3\r\n",
      "libclang==18.1.1\r\n",
      "lxml==4.5.0\r\n",
      "macaroonbakery==1.3.1\r\n",
      "Markdown==3.7\r\n",
      "MarkupSafe==2.1.5\r\n",
      "matplotlib==3.7.5\r\n",
      "matplotlib-inline==0.1.7\r\n",
      "mercantile==1.2.1\r\n",
      "more-itertools==4.2.0\r\n",
      "mpmath==1.3.0\r\n",
      "nbformat==5.10.4\r\n",
      "nest-asyncio==1.6.0\r\n",
      "netifaces==0.10.4\r\n",
      "networkx==3.1\r\n",
      "numexpr==2.7.1\r\n",
      "numpy==1.24.3\r\n",
      "nvidia-cublas-cu12==12.1.3.1\r\n",
      "nvidia-cuda-cupti-cu12==12.1.105\r\n",
      "nvidia-cuda-nvrtc-cu12==12.1.105\r\n",
      "nvidia-cuda-runtime-cu12==12.1.105\r\n",
      "nvidia-cudnn-cu12==8.9.2.26\r\n",
      "nvidia-cufft-cu12==11.0.2.54\r\n",
      "nvidia-curand-cu12==10.3.2.106\r\n",
      "nvidia-cusolver-cu12==11.4.5.107\r\n",
      "nvidia-cusparse-cu12==12.1.0.106\r\n",
      "nvidia-nccl-cu12==2.18.1\r\n",
      "nvidia-nvjitlink-cu12==12.3.101\r\n",
      "nvidia-nvtx-cu12==12.1.105\r\n",
      "oauthlib==3.1.0\r\n",
      "olefile==0.46\r\n",
      "open-clip-torch==2.24.0\r\n",
      "openpyxl==3.0.3\r\n",
      "opt-einsum==3.3.0\r\n",
      "packaging==23.2\r\n",
      "pandas==2.0.3\r\n",
      "parso==0.8.4\r\n",
      "patsy==0.5.4\r\n",
      "pexpect==4.6.0\r\n",
      "pgmpy==0.1.24\r\n",
      "pickleshare==0.7.5\r\n",
      "Pillow==8.2.0\r\n",
      "platformdirs==4.3.6\r\n",
      "pluggy==0.13.0\r\n",
      "promise==2.3\r\n",
      "prompt-toolkit==3.0.47\r\n",
      "protobuf==4.25.5\r\n",
      "psutil==6.0.0\r\n",
      "pure-eval==0.2.2\r\n",
      "py==1.8.1\r\n",
      "py4j==0.10.9.7\r\n",
      "pyasn1==0.4.2\r\n",
      "pyasn1-modules==0.2.1\r\n",
      "pycairo==1.16.2\r\n",
      "pycups==1.9.73\r\n",
      "pydot==2.0.0\r\n",
      "pygments==2.18.0\r\n",
      "PyGObject==3.36.0\r\n",
      "PyHamcrest==1.9.0\r\n",
      "pyinotify==0.9.6\r\n",
      "PyJWT==1.7.1\r\n",
      "pymacaroons==0.13.0\r\n",
      "PyNaCl==1.3.0\r\n",
      "pyOpenSSL==19.0.0\r\n",
      "pyparsing==3.1.4\r\n",
      "pyproj==3.5.0\r\n",
      "pyRFC3339==1.1\r\n",
      "pyrsistent==0.15.5\r\n",
      "pyserial==3.4\r\n",
      "pyspark==3.4.1\r\n",
      "pytest==4.6.9\r\n",
      "python-apt==2.0.1+ubuntu0.20.4.1\r\n",
      "python-dateutil==2.8.2\r\n",
      "python-debian==0.1.36+ubuntu1.1\r\n",
      "pytz==2023.3.post1\r\n",
      "PyYAML==5.3.1\r\n",
      "pyzmq==26.2.0\r\n",
      "rasterio==1.3.10\r\n",
      "regex==2024.4.28\r\n",
      "requests==2.22.0\r\n",
      "requests-oauthlib==2.0.0\r\n",
      "requests-unixsocket==0.2.0\r\n",
      "rsa==4.9\r\n",
      "safetensors==0.4.3\r\n",
      "scikit-learn==1.3.2\r\n",
      "scipy==1.8.0\r\n",
      "seaborn==0.13.2\r\n",
      "SecretStorage==2.3.1\r\n",
      "sentence-transformers==3.1.1\r\n",
      "sentencepiece==0.2.0\r\n",
      "service-identity==18.1.0\r\n",
      "shapely==2.0.6\r\n",
      "simplejson==3.16.0\r\n",
      "six==1.14.0\r\n",
      "snuggs==1.4.7\r\n",
      "sos==4.7.2\r\n",
      "soupsieve==1.9.5\r\n",
      "spark-nlp==3.4.1\r\n",
      "ssh-import-id==5.10\r\n",
      "stack-data==0.6.3\r\n",
      "statsmodels==0.14.1\r\n",
      "summarytools==0.3.0\r\n",
      "sympy==1.12\r\n",
      "systemd-python==234\r\n",
      "tables==3.6.1\r\n",
      "tabulate==0.9.0\r\n",
      "tensorboard==2.13.0\r\n",
      "tensorboard-data-server==0.7.2\r\n",
      "tensorflow==2.13.1\r\n",
      "tensorflow-datasets==4.9.2\r\n",
      "tensorflow-estimator==2.13.0\r\n",
      "tensorflow-io-gcs-filesystem==0.34.0\r\n",
      "tensorflow-metadata==1.14.0\r\n",
      "termcolor==2.4.0\r\n",
      "threadpoolctl==3.1.0\r\n",
      "timm==0.9.16\r\n",
      "tokenizers==0.19.1\r\n",
      "toml==0.10.2\r\n",
      "torch==2.1.2\r\n",
      "torchvision==0.18.0\r\n",
      "tornado==6.4.1\r\n",
      "tqdm==4.66.1\r\n",
      "traitlets==5.14.3\r\n",
      "transformers==4.40.2\r\n",
      "triton==2.1.0\r\n",
      "Twisted==18.9.0\r\n",
      "typing-extensions==4.5.0\r\n",
      "tzdata==2023.3\r\n",
      "ubuntu-pro-client==8001\r\n",
      "ufw==0.36\r\n",
      "unattended-upgrades==0.1\r\n",
      "urllib3==1.25.8\r\n",
      "visualkeras==0.1.4\r\n",
      "wadllib==1.3.3\r\n",
      "wcwidth==0.2.13\r\n",
      "webencodings==0.5.1\r\n",
      "werkzeug==3.0.6\r\n",
      "wrapt==1.17.0\r\n",
      "xlrd==1.1.0\r\n",
      "xlwt==1.3.0\r\n",
      "xyzservices==2024.6.0\r\n",
      "zipp==3.20.2\r\n",
      "zope.interface==4.7.1\r\n"
     ]
    }
   ],
   "source": [
    "# show installed packages and versions\n",
    "!pip freeze"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setting PATH correctly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['/home/smohr001/thesis',\n",
       " '/usr/lib/python38.zip',\n",
       " '/usr/lib/python3.8',\n",
       " '/usr/lib/python3.8/lib-dynload',\n",
       " '',\n",
       " '/opt/jupyterhub/lib/python3.8/site-packages',\n",
       " '/opt/jupyterhub/lib/python3.8/site-packages/IPython/extensions',\n",
       " '/home/smohr001/.ipython',\n",
       " '/home/smohr001/.local/lib/python3.8/site-packages']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# there is somewhere a PATH-error on LENA for a while\n",
    "# adding my packages path to the PATH environment\n",
    "\n",
    "import sys\n",
    "sys.path.append(\"/home/smohr001/.local/lib/python3.8/site-packages\")\n",
    "sys.path"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Some important installed libraries:\n",
      "\n",
      "Pandas version: 1.4.1\n",
      "Numpy version: 1.22.2\n",
      "Seaborn version: 0.13.2\n"
     ]
    }
   ],
   "source": [
    "# importing standard libraries\n",
    "import sys\n",
    "import os\n",
    "import warnings\n",
    "import datetime\n",
    "import time\n",
    "import math\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import random\n",
    "import logging\n",
    "\n",
    "# importing shared procedures for this procect (needs to be a simple .py file)\n",
    "%run shared_procedures.py\n",
    "\n",
    "# importing additional libraries\n",
    "import requests\n",
    "from requests.exceptions import HTTPError\n",
    "import json\n",
    "\n",
    "# get info about installed and used versions of some important (deep learning) libraries\n",
    "print(\"Some important installed libraries:\\n\")\n",
    "print(f\"Pandas version: {pd.__version__}\")\n",
    "print(f\"Numpy version: {np.__version__}\")\n",
    "print(f\"Seaborn version: {sns.__version__}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Set up parameters and identification of this script"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-01-10 12:57:33,731 - INFO - Starting script '10_scrape_earthquake_data.ipynb'.\n",
      "2025-01-10 12:57:33,733 - INFO - Set loglevel to INFO.\n",
      "2025-01-10 12:57:33,734 - INFO - 10_scrape_earthquake_data.ipynb: Set data directory to './data'.\n"
     ]
    }
   ],
   "source": [
    "# show all matplotlib graphs inline\n",
    "%matplotlib inline\n",
    "\n",
    "# ignore warnings (low priority)\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# set script (ipynb notebook) name (e.g. for logging)\n",
    "script_name = \"10_scrape_earthquake_data.ipynb\"\n",
    "\n",
    "# start parameterized logging\n",
    "setup_logging(logfile_dir = \"log\", \n",
    "              logfile_name = \"10_data_scraping.log\", \n",
    "              log_level = logging.INFO, \n",
    "              script_name = script_name\n",
    "             )\n",
    "\n",
    "# set data directory\n",
    "data_dir = \"data\"\n",
    "logging.info(f\"{script_name}: Set data directory to './{data_dir}'.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Testing the API data availability and showing the geoJSON based result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "geoJSON format\n",
      "\n",
      "{\"type\":\"FeatureCollection\",\"metadata\":{\"generated\":1736513859000,\"url\":\"https://earthquake.usgs.gov/fdsnws/event/1/query?format=geojson&starttime=2014-01-01&endtime=2014-01-02&minmagnitude=6\",\"title\":\"USGS Earthquakes\",\"status\":200,\"api\":\"1.14.1\",\"count\":1},\"features\":[{\"type\":\"Feature\",\"properties\":{\"mag\":6.5,\"place\":\"32 km W of Sola, Vanuatu\",\"time\":1388592209000,\"updated\":1651596180609,\"tz\":null,\"url\":\"https://earthquake.usgs.gov/earthquakes/eventpage/usc000lvb5\",\"detail\":\"https://earthquake.usgs.gov/fdsnws/event/1/query?eventid=usc000lvb5&format=geojson\",\"felt\":null,\"cdi\":null,\"mmi\":4.262,\"alert\":\"green\",\"status\":\"reviewed\",\"tsunami\":1,\"sig\":650,\"net\":\"us\",\"code\":\"c000lvb5\",\"ids\":\",pt14001000,at00myqcls,usc000lvb5,iscgem604060577,\",\"sources\":\",pt,at,us,iscgem,\",\"types\":\",cap,impact-link,losspager,moment-tensor,origin,phase-data,shakemap,\",\"nst\":null,\"dmin\":3.997,\"rms\":0.76,\"gap\":14,\"magType\":\"mww\",\"type\":\"earthquake\",\"title\":\"M 6.5 - 32 km W of Sola, Vanuatu\"},\"geometry\":{\"type\":\"Point\",\"coordinates\":[167.249,-13.8633,187]},\"id\":\"usc000lvb5\"}]}\n"
     ]
    }
   ],
   "source": [
    "# geoJSON format of answer\n",
    "# should result in excatly 1 earthquake\n",
    "query_parameters = {\n",
    "    \"format\": \"geojson\",\n",
    "    \"starttime\": \"2014-01-01\",\n",
    "    \"endtime\": \"2014-01-02\",\n",
    "    \"minmagnitude\": 6\n",
    "}\n",
    "\n",
    "query_status, query_answer = get_data_from_web_api(url = \"https://earthquake.usgs.gov/fdsnws/event/1/query?\",\n",
    "                                                        query_parameters = query_parameters,\n",
    "                                                        verbosity = 0)\n",
    "\n",
    "if(query_status):\n",
    "    print(\"geoJSON format\\n\")\n",
    "    print(query_answer.text)\n",
    "else:\n",
    "    print(\"\\nSome error occured! Nothing to print!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Main query method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def query_earthquakes(query_timeframe, api_query_parameters, verbosity=0):\n",
    "\n",
    "    \"\"\"\n",
    "    Queries the United States Geological Survey (USGS) Advanced National Seismic System (ANSS) API for earthquake data \n",
    "    and returns a pandas DataFrame containing the results.\n",
    "\n",
    "    Parameters:\n",
    "        query_timeframe : list\n",
    "            A list of timeframes to query within. The list should be in a format that the API endpoint accepts.\n",
    "        api_query_parameters : dict\n",
    "            A dictionary of query parameters to be sent to the API endpoint. Parameters include but are not limited to \n",
    "            'starttime', 'endtime', 'minmagnitude', and 'maxmagnitude'.\n",
    "        verbosity : int, optional\n",
    "            Verbosity level for logging and printing. Default is 0 (no verbose output).\n",
    "\n",
    "    Returns:\n",
    "        pandas.DataFrame\n",
    "            A DataFrame containing the requested earthquake data. The columns typically include 'id', 'lon', 'lat', 'depth',\n",
    "            'mag', 'time', 'felt', 'cdi', 'mmi', 'alert', 'status', 'tsunami', 'nst', 'net', and 'sig'.\n",
    "    \n",
    "    Raises:\n",
    "        ValueError\n",
    "            If 'query_timeframe' or 'api_query_parameters' are not set or are invalid.\n",
    "        RuntimeError\n",
    "            If the API query fails to retrieve data.\n",
    "    \n",
    "    Logs:\n",
    "        Logs the start and end of the querying process, error encountered during query and data retrieval,\n",
    "        and information about the number of events queried and parsed.\n",
    "    \n",
    "    Notes:\n",
    "        This docstring was generated with the help of AI and proofread by the author.\n",
    "    \"\"\"\n",
    "    \n",
    "    # are some parameters set?\n",
    "    if (query_timeframe and api_query_parameters):\n",
    "        \n",
    "        # print & log some information\n",
    "        print(\"======================================================================================================\")\n",
    "        print(\"Querying United States Geological Survey (USGS) Advanced National Seismic System (ANSS)\")\n",
    "        logging.info(f\"query_earthquakes: START main query method for earthquakes.\")\n",
    "\n",
    "        # initialize timing information for this routine\n",
    "        start = time.time()\n",
    "\n",
    "        # initialize empty dataframe earthquakes\n",
    "        earthquakes = pd.DataFrame()\n",
    "\n",
    "        # start  of loop -----------------------------------------------------------------------------------------------\n",
    "        # simulating a repeat-until-loop structure\n",
    "        while True:\n",
    "\n",
    "            # adding starttime and endtime to the query parameters\n",
    "            api_query_parameters[\"starttime\"] = query_timeframe[0]\n",
    "            api_query_parameters[\"endtime\"] = query_timeframe[1]\n",
    "\n",
    "            # show the parameters\n",
    "            print(\"\\nTimeframe(s): \" + str(query_timeframe))\n",
    "            print(\"\\nQuery parameters: \" + str(api_query_parameters))\n",
    "\n",
    "            # get the number of events to check the threshold of allowed events (show errors, verbosity = 1)\n",
    "            query_status, events_count = get_data_from_web_api(url = \"https://earthquake.usgs.gov/fdsnws/event/1/count?\",\n",
    "                                                                     query_parameters = api_query_parameters,\n",
    "                                                                     verbosity = verbosity)\n",
    "\n",
    "            # parse the queried data (to get the number of events)\n",
    "            if(query_status):\n",
    "                count = events_count.json()['count']\n",
    "                maxAllowed = events_count.json()['maxAllowed']\n",
    "\n",
    "                # set number of events to query per API query well below the API threshold \n",
    "                if(maxAllowed > 5000):\n",
    "                    maxAllowed = 5000\n",
    "\n",
    "                # print some info for the number of events\n",
    "                print(\"Number of events to query: \" + str(count) + \" (of \" + str(maxAllowed) + \" allowed)\")\n",
    "\n",
    "                # check the number of events for the given threshold\n",
    "                if(count <= maxAllowed):\n",
    "                    # number of events is below threshold\n",
    "                    print(\"Number of events is below threshold, getting earthquake data ...\")\n",
    "\n",
    "                    # query the API (show errors, verbosity = 1)\n",
    "                    query_status_ok, api_response_json = \\\n",
    "                        get_data_from_web_api(url = \"https://earthquake.usgs.gov/fdsnws/event/1/query?\",\n",
    "                                                    query_parameters = api_query_parameters,\n",
    "                                                    verbosity = verbosity)\n",
    "\n",
    "                    # parse the queried data\n",
    "                    if(query_status_ok):\n",
    "                        # query should be okay, go ahead\n",
    "                        print(\"Got data, parsing ...\")\n",
    "\n",
    "                        # parse the 'FeatureCollection' from geoJSON request answer\n",
    "                        feature_collection = api_response_json.json()['metadata']\n",
    "                        count = feature_collection['count']\n",
    "                        print(\"Number of events to parse:\", count)\n",
    "\n",
    "                        # get features branch of JSON\n",
    "                        features = api_response_json.json()['features']\n",
    "\n",
    "                        # run through every feature (which is one event = earthquake)\n",
    "                        for feature in features:\n",
    "                            # set an empty featurelist for THIS earthquake\n",
    "                            earthquake = []\n",
    "\n",
    "                            # get usgs id\n",
    "                            earthquake.append(feature['id'])\n",
    "\n",
    "                            # get geometry and coordinates (a list) and lat, lon and depth\n",
    "                            coordinates = feature['geometry']['coordinates']\n",
    "                            earthquake.append(coordinates[0])\n",
    "                            earthquake.append(coordinates[1])\n",
    "                            earthquake.append(coordinates[2])\n",
    "\n",
    "                            # get all properties of this earthquake\n",
    "                            properties = feature['properties']\n",
    "                            for key in properties:\n",
    "                                # _drop_ some unwanted properties \n",
    "                                if(key not in ('rms','dmin','gap','magType','url','detail','code','ids','sources', \\\n",
    "                                               'types','title','place','type','updated','tz')):\n",
    "                                    # get the key-value-pair without any conversion or formatting\n",
    "                                    earthquake.append(properties[key])\n",
    "\n",
    "                            # append this earhquake to the dataframe of earthquakes\n",
    "                            earthquake_df = pd.DataFrame([earthquake])\n",
    "                            earthquakes = pd.concat([earthquakes, earthquake_df])\n",
    "\n",
    "                    else:\n",
    "                        # unsuccessful query \n",
    "                        logging.error(f\"query_earthquakes: Nothing to parse!\")\n",
    "\n",
    "                    # delete first timeframe (to continue with the next one)\n",
    "                    query_timeframe.pop(0)\n",
    "\n",
    "                    # set count to 200001 to simply CONTINUE with the main loop\n",
    "                    count = maxAllowed + 1;\n",
    "\n",
    "\n",
    "                else:\n",
    "                    # number of events too high\n",
    "                    print(\"Number of events is too high, setting a reduced time frame!\")\n",
    "                    start_time = datetime.datetime.strptime(api_query_parameters['starttime'], '%Y-%m-%d %H:%M:%S')\n",
    "                    end_time = datetime.datetime.strptime(api_query_parameters['endtime'], '%Y-%m-%d %H:%M:%S')\n",
    "                    time_diff = (end_time - start_time).total_seconds()\n",
    "                    new_end_time = start_time + datetime.timedelta(0,int(time_diff / 2))\n",
    "                    query_timeframe.insert(1, str(new_end_time))\n",
    "                    print(\"New timeframe(s): \" + str(query_timeframe))\n",
    "\n",
    "            else:\n",
    "                # unsuccessful query \n",
    "                logging.error(f\"query_earthquakes: Bad query, nothing to parse!\")\n",
    "\n",
    "                # set count = 0 and maxAllowed = 1 to simply exit the main loop\n",
    "                count = 0\n",
    "                maxAllowed = 1\n",
    "\n",
    "            # exit the main loop if\n",
    "            #   (1) less than the maxAllowed objects will be queried or\n",
    "            #   (2) the last timeframe has been reached\n",
    "            #   (3) an unsuccessful query has been carried out\n",
    "            if ((count < maxAllowed) or (len(query_timeframe) < 2)):\n",
    "                break\n",
    "\n",
    "        # end of loop -----------------------------------------------------------------------------------------------\n",
    "        \n",
    "        # rename the columns of the earthquakes dataframe (if there are any)\n",
    "        if(len(earthquakes) > 0):\n",
    "            earthquakes.columns = [\"id\",\"lon\",\"lat\",\"depth\",\"mag\",\"time\",\"felt\",\"cdi\",\"mmi\",\"alert\",\"status\",\"tsunami\",\"nst\",\"net\",\"sig\"]\n",
    "        \n",
    "        # print & log some final information (numer of parsed events and runtime of routine)\n",
    "        print(\"------------------------------------------------------------------------------------------------------\")\n",
    "        print(\"\\nTotal number of parsed events in dataframe: \" + str(len(earthquakes)))\n",
    "\n",
    "        end = time.time()\n",
    "        print(\"Runtime to query and parse the data: \" + str(round(end - start, 1)) + \" s\")\n",
    "        print(\"======================================================================================================\")\n",
    "        time.sleep(0.5)\n",
    "        logging.info(f\"query_earthquakes: END main query method for earthquakes with {len(earthquakes)} earthquakes in {round(end - start, 1)} s.\")\n",
    "        \n",
    "        # reset index\n",
    "        earthquakes.reset_index(drop=True, inplace=True)\n",
    "\n",
    "        # return the dataframe with found earthquakes\n",
    "        return earthquakes\n",
    "\n",
    "    # no input parameters evailable\n",
    "    else:\n",
    "        logging.error(f\"query_earthquakes: No input parameters evailable!\")\n",
    "        return(None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Querying earthquake data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-01-10 12:58:10,402 - INFO - query_earthquakes: START main query method for earthquakes.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================================================\n",
      "Querying United States Geological Survey (USGS) Advanced National Seismic System (ANSS)\n",
      "\n",
      "Timeframe(s): ['1970-01-01 00:00:00', '2019-12-31 23:59:59']\n",
      "\n",
      "Query parameters: {'minlatitude': '-70.0', 'maxlatitude': '70.0', 'minlongitude': '100.0', 'maxlongitude': '300.0', 'format': 'geojson', 'minmagnitude': '5.0', 'eventtype': 'earthquake', 'starttime': '1970-01-01 00:00:00', 'endtime': '2019-12-31 23:59:59'}\n",
      "Number of events to query: 63878 (of 5000 allowed)\n",
      "Number of events is too high, setting a reduced time frame!\n",
      "New timeframe(s): ['1970-01-01 00:00:00', '1994-12-31 23:59:59', '2019-12-31 23:59:59']\n",
      "\n",
      "Timeframe(s): ['1970-01-01 00:00:00', '1994-12-31 23:59:59', '2019-12-31 23:59:59']\n",
      "\n",
      "Query parameters: {'minlatitude': '-70.0', 'maxlatitude': '70.0', 'minlongitude': '100.0', 'maxlongitude': '300.0', 'format': 'geojson', 'minmagnitude': '5.0', 'eventtype': 'earthquake', 'starttime': '1970-01-01 00:00:00', 'endtime': '1994-12-31 23:59:59'}\n",
      "Number of events to query: 30622 (of 5000 allowed)\n",
      "Number of events is too high, setting a reduced time frame!\n",
      "New timeframe(s): ['1970-01-01 00:00:00', '1982-07-02 11:59:59', '1994-12-31 23:59:59', '2019-12-31 23:59:59']\n",
      "\n",
      "Timeframe(s): ['1970-01-01 00:00:00', '1982-07-02 11:59:59', '1994-12-31 23:59:59', '2019-12-31 23:59:59']\n",
      "\n",
      "Query parameters: {'minlatitude': '-70.0', 'maxlatitude': '70.0', 'minlongitude': '100.0', 'maxlongitude': '300.0', 'format': 'geojson', 'minmagnitude': '5.0', 'eventtype': 'earthquake', 'starttime': '1970-01-01 00:00:00', 'endtime': '1982-07-02 11:59:59'}\n",
      "Number of events to query: 13309 (of 5000 allowed)\n",
      "Number of events is too high, setting a reduced time frame!\n",
      "New timeframe(s): ['1970-01-01 00:00:00', '1976-04-01 17:59:59', '1982-07-02 11:59:59', '1994-12-31 23:59:59', '2019-12-31 23:59:59']\n",
      "\n",
      "Timeframe(s): ['1970-01-01 00:00:00', '1976-04-01 17:59:59', '1982-07-02 11:59:59', '1994-12-31 23:59:59', '2019-12-31 23:59:59']\n",
      "\n",
      "Query parameters: {'minlatitude': '-70.0', 'maxlatitude': '70.0', 'minlongitude': '100.0', 'maxlongitude': '300.0', 'format': 'geojson', 'minmagnitude': '5.0', 'eventtype': 'earthquake', 'starttime': '1970-01-01 00:00:00', 'endtime': '1976-04-01 17:59:59'}\n",
      "Number of events to query: 5372 (of 5000 allowed)\n",
      "Number of events is too high, setting a reduced time frame!\n",
      "New timeframe(s): ['1970-01-01 00:00:00', '1973-02-15 08:59:59', '1976-04-01 17:59:59', '1982-07-02 11:59:59', '1994-12-31 23:59:59', '2019-12-31 23:59:59']\n",
      "\n",
      "Timeframe(s): ['1970-01-01 00:00:00', '1973-02-15 08:59:59', '1976-04-01 17:59:59', '1982-07-02 11:59:59', '1994-12-31 23:59:59', '2019-12-31 23:59:59']\n",
      "\n",
      "Query parameters: {'minlatitude': '-70.0', 'maxlatitude': '70.0', 'minlongitude': '100.0', 'maxlongitude': '300.0', 'format': 'geojson', 'minmagnitude': '5.0', 'eventtype': 'earthquake', 'starttime': '1970-01-01 00:00:00', 'endtime': '1973-02-15 08:59:59'}\n",
      "Number of events to query: 1492 (of 5000 allowed)\n",
      "Number of events is below threshold, getting earthquake data ...\n",
      "Got data, parsing ...\n",
      "Number of events to parse: 1492\n",
      "\n",
      "Timeframe(s): ['1973-02-15 08:59:59', '1976-04-01 17:59:59', '1982-07-02 11:59:59', '1994-12-31 23:59:59', '2019-12-31 23:59:59']\n",
      "\n",
      "Query parameters: {'minlatitude': '-70.0', 'maxlatitude': '70.0', 'minlongitude': '100.0', 'maxlongitude': '300.0', 'format': 'geojson', 'minmagnitude': '5.0', 'eventtype': 'earthquake', 'starttime': '1973-02-15 08:59:59', 'endtime': '1976-04-01 17:59:59'}\n",
      "Number of events to query: 3880 (of 5000 allowed)\n",
      "Number of events is below threshold, getting earthquake data ...\n",
      "Got data, parsing ...\n",
      "Number of events to parse: 3880\n",
      "\n",
      "Timeframe(s): ['1976-04-01 17:59:59', '1982-07-02 11:59:59', '1994-12-31 23:59:59', '2019-12-31 23:59:59']\n",
      "\n",
      "Query parameters: {'minlatitude': '-70.0', 'maxlatitude': '70.0', 'minlongitude': '100.0', 'maxlongitude': '300.0', 'format': 'geojson', 'minmagnitude': '5.0', 'eventtype': 'earthquake', 'starttime': '1976-04-01 17:59:59', 'endtime': '1982-07-02 11:59:59'}\n",
      "Number of events to query: 7937 (of 5000 allowed)\n",
      "Number of events is too high, setting a reduced time frame!\n",
      "New timeframe(s): ['1976-04-01 17:59:59', '1979-05-18 02:59:59', '1982-07-02 11:59:59', '1994-12-31 23:59:59', '2019-12-31 23:59:59']\n",
      "\n",
      "Timeframe(s): ['1976-04-01 17:59:59', '1979-05-18 02:59:59', '1982-07-02 11:59:59', '1994-12-31 23:59:59', '2019-12-31 23:59:59']\n",
      "\n",
      "Query parameters: {'minlatitude': '-70.0', 'maxlatitude': '70.0', 'minlongitude': '100.0', 'maxlongitude': '300.0', 'format': 'geojson', 'minmagnitude': '5.0', 'eventtype': 'earthquake', 'starttime': '1976-04-01 17:59:59', 'endtime': '1979-05-18 02:59:59'}\n",
      "Number of events to query: 4383 (of 5000 allowed)\n",
      "Number of events is below threshold, getting earthquake data ...\n",
      "Got data, parsing ...\n",
      "Number of events to parse: 4383\n",
      "\n",
      "Timeframe(s): ['1979-05-18 02:59:59', '1982-07-02 11:59:59', '1994-12-31 23:59:59', '2019-12-31 23:59:59']\n",
      "\n",
      "Query parameters: {'minlatitude': '-70.0', 'maxlatitude': '70.0', 'minlongitude': '100.0', 'maxlongitude': '300.0', 'format': 'geojson', 'minmagnitude': '5.0', 'eventtype': 'earthquake', 'starttime': '1979-05-18 02:59:59', 'endtime': '1982-07-02 11:59:59'}\n",
      "Number of events to query: 3554 (of 5000 allowed)\n",
      "Number of events is below threshold, getting earthquake data ...\n",
      "Got data, parsing ...\n",
      "Number of events to parse: 3554\n",
      "\n",
      "Timeframe(s): ['1982-07-02 11:59:59', '1994-12-31 23:59:59', '2019-12-31 23:59:59']\n",
      "\n",
      "Query parameters: {'minlatitude': '-70.0', 'maxlatitude': '70.0', 'minlongitude': '100.0', 'maxlongitude': '300.0', 'format': 'geojson', 'minmagnitude': '5.0', 'eventtype': 'earthquake', 'starttime': '1982-07-02 11:59:59', 'endtime': '1994-12-31 23:59:59'}\n",
      "Number of events to query: 17313 (of 5000 allowed)\n",
      "Number of events is too high, setting a reduced time frame!\n",
      "New timeframe(s): ['1982-07-02 11:59:59', '1988-10-01 05:59:59', '1994-12-31 23:59:59', '2019-12-31 23:59:59']\n",
      "\n",
      "Timeframe(s): ['1982-07-02 11:59:59', '1988-10-01 05:59:59', '1994-12-31 23:59:59', '2019-12-31 23:59:59']\n",
      "\n",
      "Query parameters: {'minlatitude': '-70.0', 'maxlatitude': '70.0', 'minlongitude': '100.0', 'maxlongitude': '300.0', 'format': 'geojson', 'minmagnitude': '5.0', 'eventtype': 'earthquake', 'starttime': '1982-07-02 11:59:59', 'endtime': '1988-10-01 05:59:59'}\n",
      "Number of events to query: 8792 (of 5000 allowed)\n",
      "Number of events is too high, setting a reduced time frame!\n",
      "New timeframe(s): ['1982-07-02 11:59:59', '1985-08-16 20:59:59', '1988-10-01 05:59:59', '1994-12-31 23:59:59', '2019-12-31 23:59:59']\n",
      "\n",
      "Timeframe(s): ['1982-07-02 11:59:59', '1985-08-16 20:59:59', '1988-10-01 05:59:59', '1994-12-31 23:59:59', '2019-12-31 23:59:59']\n",
      "\n",
      "Query parameters: {'minlatitude': '-70.0', 'maxlatitude': '70.0', 'minlongitude': '100.0', 'maxlongitude': '300.0', 'format': 'geojson', 'minmagnitude': '5.0', 'eventtype': 'earthquake', 'starttime': '1982-07-02 11:59:59', 'endtime': '1985-08-16 20:59:59'}\n",
      "Number of events to query: 4407 (of 5000 allowed)\n",
      "Number of events is below threshold, getting earthquake data ...\n",
      "Got data, parsing ...\n",
      "Number of events to parse: 4407\n",
      "\n",
      "Timeframe(s): ['1985-08-16 20:59:59', '1988-10-01 05:59:59', '1994-12-31 23:59:59', '2019-12-31 23:59:59']\n",
      "\n",
      "Query parameters: {'minlatitude': '-70.0', 'maxlatitude': '70.0', 'minlongitude': '100.0', 'maxlongitude': '300.0', 'format': 'geojson', 'minmagnitude': '5.0', 'eventtype': 'earthquake', 'starttime': '1985-08-16 20:59:59', 'endtime': '1988-10-01 05:59:59'}\n",
      "Number of events to query: 4385 (of 5000 allowed)\n",
      "Number of events is below threshold, getting earthquake data ...\n",
      "Got data, parsing ...\n",
      "Number of events to parse: 4385\n",
      "\n",
      "Timeframe(s): ['1988-10-01 05:59:59', '1994-12-31 23:59:59', '2019-12-31 23:59:59']\n",
      "\n",
      "Query parameters: {'minlatitude': '-70.0', 'maxlatitude': '70.0', 'minlongitude': '100.0', 'maxlongitude': '300.0', 'format': 'geojson', 'minmagnitude': '5.0', 'eventtype': 'earthquake', 'starttime': '1988-10-01 05:59:59', 'endtime': '1994-12-31 23:59:59'}\n",
      "Number of events to query: 8521 (of 5000 allowed)\n",
      "Number of events is too high, setting a reduced time frame!\n",
      "New timeframe(s): ['1988-10-01 05:59:59', '1991-11-16 14:59:59', '1994-12-31 23:59:59', '2019-12-31 23:59:59']\n",
      "\n",
      "Timeframe(s): ['1988-10-01 05:59:59', '1991-11-16 14:59:59', '1994-12-31 23:59:59', '2019-12-31 23:59:59']\n",
      "\n",
      "Query parameters: {'minlatitude': '-70.0', 'maxlatitude': '70.0', 'minlongitude': '100.0', 'maxlongitude': '300.0', 'format': 'geojson', 'minmagnitude': '5.0', 'eventtype': 'earthquake', 'starttime': '1988-10-01 05:59:59', 'endtime': '1991-11-16 14:59:59'}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of events to query: 4196 (of 5000 allowed)\n",
      "Number of events is below threshold, getting earthquake data ...\n",
      "Got data, parsing ...\n",
      "Number of events to parse: 4196\n",
      "\n",
      "Timeframe(s): ['1991-11-16 14:59:59', '1994-12-31 23:59:59', '2019-12-31 23:59:59']\n",
      "\n",
      "Query parameters: {'minlatitude': '-70.0', 'maxlatitude': '70.0', 'minlongitude': '100.0', 'maxlongitude': '300.0', 'format': 'geojson', 'minmagnitude': '5.0', 'eventtype': 'earthquake', 'starttime': '1991-11-16 14:59:59', 'endtime': '1994-12-31 23:59:59'}\n",
      "Number of events to query: 4325 (of 5000 allowed)\n",
      "Number of events is below threshold, getting earthquake data ...\n",
      "Got data, parsing ...\n",
      "Number of events to parse: 4325\n",
      "\n",
      "Timeframe(s): ['1994-12-31 23:59:59', '2019-12-31 23:59:59']\n",
      "\n",
      "Query parameters: {'minlatitude': '-70.0', 'maxlatitude': '70.0', 'minlongitude': '100.0', 'maxlongitude': '300.0', 'format': 'geojson', 'minmagnitude': '5.0', 'eventtype': 'earthquake', 'starttime': '1994-12-31 23:59:59', 'endtime': '2019-12-31 23:59:59'}\n",
      "Number of events to query: 33256 (of 5000 allowed)\n",
      "Number of events is too high, setting a reduced time frame!\n",
      "New timeframe(s): ['1994-12-31 23:59:59', '2007-07-02 11:59:59', '2019-12-31 23:59:59']\n",
      "\n",
      "Timeframe(s): ['1994-12-31 23:59:59', '2007-07-02 11:59:59', '2019-12-31 23:59:59']\n",
      "\n",
      "Query parameters: {'minlatitude': '-70.0', 'maxlatitude': '70.0', 'minlongitude': '100.0', 'maxlongitude': '300.0', 'format': 'geojson', 'minmagnitude': '5.0', 'eventtype': 'earthquake', 'starttime': '1994-12-31 23:59:59', 'endtime': '2007-07-02 11:59:59'}\n",
      "Number of events to query: 14366 (of 5000 allowed)\n",
      "Number of events is too high, setting a reduced time frame!\n",
      "New timeframe(s): ['1994-12-31 23:59:59', '2001-04-01 17:59:59', '2007-07-02 11:59:59', '2019-12-31 23:59:59']\n",
      "\n",
      "Timeframe(s): ['1994-12-31 23:59:59', '2001-04-01 17:59:59', '2007-07-02 11:59:59', '2019-12-31 23:59:59']\n",
      "\n",
      "Query parameters: {'minlatitude': '-70.0', 'maxlatitude': '70.0', 'minlongitude': '100.0', 'maxlongitude': '300.0', 'format': 'geojson', 'minmagnitude': '5.0', 'eventtype': 'earthquake', 'starttime': '1994-12-31 23:59:59', 'endtime': '2001-04-01 17:59:59'}\n",
      "Number of events to query: 6842 (of 5000 allowed)\n",
      "Number of events is too high, setting a reduced time frame!\n",
      "New timeframe(s): ['1994-12-31 23:59:59', '1998-02-15 08:59:59', '2001-04-01 17:59:59', '2007-07-02 11:59:59', '2019-12-31 23:59:59']\n",
      "\n",
      "Timeframe(s): ['1994-12-31 23:59:59', '1998-02-15 08:59:59', '2001-04-01 17:59:59', '2007-07-02 11:59:59', '2019-12-31 23:59:59']\n",
      "\n",
      "Query parameters: {'minlatitude': '-70.0', 'maxlatitude': '70.0', 'minlongitude': '100.0', 'maxlongitude': '300.0', 'format': 'geojson', 'minmagnitude': '5.0', 'eventtype': 'earthquake', 'starttime': '1994-12-31 23:59:59', 'endtime': '1998-02-15 08:59:59'}\n",
      "Number of events to query: 3564 (of 5000 allowed)\n",
      "Number of events is below threshold, getting earthquake data ...\n",
      "Got data, parsing ...\n",
      "Number of events to parse: 3564\n",
      "\n",
      "Timeframe(s): ['1998-02-15 08:59:59', '2001-04-01 17:59:59', '2007-07-02 11:59:59', '2019-12-31 23:59:59']\n",
      "\n",
      "Query parameters: {'minlatitude': '-70.0', 'maxlatitude': '70.0', 'minlongitude': '100.0', 'maxlongitude': '300.0', 'format': 'geojson', 'minmagnitude': '5.0', 'eventtype': 'earthquake', 'starttime': '1998-02-15 08:59:59', 'endtime': '2001-04-01 17:59:59'}\n",
      "Number of events to query: 3278 (of 5000 allowed)\n",
      "Number of events is below threshold, getting earthquake data ...\n",
      "Got data, parsing ...\n",
      "Number of events to parse: 3278\n",
      "\n",
      "Timeframe(s): ['2001-04-01 17:59:59', '2007-07-02 11:59:59', '2019-12-31 23:59:59']\n",
      "\n",
      "Query parameters: {'minlatitude': '-70.0', 'maxlatitude': '70.0', 'minlongitude': '100.0', 'maxlongitude': '300.0', 'format': 'geojson', 'minmagnitude': '5.0', 'eventtype': 'earthquake', 'starttime': '2001-04-01 17:59:59', 'endtime': '2007-07-02 11:59:59'}\n",
      "Number of events to query: 7524 (of 5000 allowed)\n",
      "Number of events is too high, setting a reduced time frame!\n",
      "New timeframe(s): ['2001-04-01 17:59:59', '2004-05-17 02:59:59', '2007-07-02 11:59:59', '2019-12-31 23:59:59']\n",
      "\n",
      "Timeframe(s): ['2001-04-01 17:59:59', '2004-05-17 02:59:59', '2007-07-02 11:59:59', '2019-12-31 23:59:59']\n",
      "\n",
      "Query parameters: {'minlatitude': '-70.0', 'maxlatitude': '70.0', 'minlongitude': '100.0', 'maxlongitude': '300.0', 'format': 'geojson', 'minmagnitude': '5.0', 'eventtype': 'earthquake', 'starttime': '2001-04-01 17:59:59', 'endtime': '2004-05-17 02:59:59'}\n",
      "Number of events to query: 3407 (of 5000 allowed)\n",
      "Number of events is below threshold, getting earthquake data ...\n",
      "Got data, parsing ...\n",
      "Number of events to parse: 3407\n",
      "\n",
      "Timeframe(s): ['2004-05-17 02:59:59', '2007-07-02 11:59:59', '2019-12-31 23:59:59']\n",
      "\n",
      "Query parameters: {'minlatitude': '-70.0', 'maxlatitude': '70.0', 'minlongitude': '100.0', 'maxlongitude': '300.0', 'format': 'geojson', 'minmagnitude': '5.0', 'eventtype': 'earthquake', 'starttime': '2004-05-17 02:59:59', 'endtime': '2007-07-02 11:59:59'}\n",
      "Number of events to query: 4117 (of 5000 allowed)\n",
      "Number of events is below threshold, getting earthquake data ...\n",
      "Got data, parsing ...\n",
      "Number of events to parse: 4117\n",
      "\n",
      "Timeframe(s): ['2007-07-02 11:59:59', '2019-12-31 23:59:59']\n",
      "\n",
      "Query parameters: {'minlatitude': '-70.0', 'maxlatitude': '70.0', 'minlongitude': '100.0', 'maxlongitude': '300.0', 'format': 'geojson', 'minmagnitude': '5.0', 'eventtype': 'earthquake', 'starttime': '2007-07-02 11:59:59', 'endtime': '2019-12-31 23:59:59'}\n",
      "Number of events to query: 18890 (of 5000 allowed)\n",
      "Number of events is too high, setting a reduced time frame!\n",
      "New timeframe(s): ['2007-07-02 11:59:59', '2013-10-01 05:59:59', '2019-12-31 23:59:59']\n",
      "\n",
      "Timeframe(s): ['2007-07-02 11:59:59', '2013-10-01 05:59:59', '2019-12-31 23:59:59']\n",
      "\n",
      "Query parameters: {'minlatitude': '-70.0', 'maxlatitude': '70.0', 'minlongitude': '100.0', 'maxlongitude': '300.0', 'format': 'geojson', 'minmagnitude': '5.0', 'eventtype': 'earthquake', 'starttime': '2007-07-02 11:59:59', 'endtime': '2013-10-01 05:59:59'}\n",
      "Number of events to query: 10769 (of 5000 allowed)\n",
      "Number of events is too high, setting a reduced time frame!\n",
      "New timeframe(s): ['2007-07-02 11:59:59', '2010-08-16 20:59:59', '2013-10-01 05:59:59', '2019-12-31 23:59:59']\n",
      "\n",
      "Timeframe(s): ['2007-07-02 11:59:59', '2010-08-16 20:59:59', '2013-10-01 05:59:59', '2019-12-31 23:59:59']\n",
      "\n",
      "Query parameters: {'minlatitude': '-70.0', 'maxlatitude': '70.0', 'minlongitude': '100.0', 'maxlongitude': '300.0', 'format': 'geojson', 'minmagnitude': '5.0', 'eventtype': 'earthquake', 'starttime': '2007-07-02 11:59:59', 'endtime': '2010-08-16 20:59:59'}\n",
      "Number of events to query: 5456 (of 5000 allowed)\n",
      "Number of events is too high, setting a reduced time frame!\n",
      "New timeframe(s): ['2007-07-02 11:59:59', '2009-01-23 04:29:59', '2010-08-16 20:59:59', '2013-10-01 05:59:59', '2019-12-31 23:59:59']\n",
      "\n",
      "Timeframe(s): ['2007-07-02 11:59:59', '2009-01-23 04:29:59', '2010-08-16 20:59:59', '2013-10-01 05:59:59', '2019-12-31 23:59:59']\n",
      "\n",
      "Query parameters: {'minlatitude': '-70.0', 'maxlatitude': '70.0', 'minlongitude': '100.0', 'maxlongitude': '300.0', 'format': 'geojson', 'minmagnitude': '5.0', 'eventtype': 'earthquake', 'starttime': '2007-07-02 11:59:59', 'endtime': '2009-01-23 04:29:59'}\n",
      "Number of events to query: 2619 (of 5000 allowed)\n",
      "Number of events is below threshold, getting earthquake data ...\n",
      "Got data, parsing ...\n",
      "Number of events to parse: 2619\n",
      "\n",
      "Timeframe(s): ['2009-01-23 04:29:59', '2010-08-16 20:59:59', '2013-10-01 05:59:59', '2019-12-31 23:59:59']\n",
      "\n",
      "Query parameters: {'minlatitude': '-70.0', 'maxlatitude': '70.0', 'minlongitude': '100.0', 'maxlongitude': '300.0', 'format': 'geojson', 'minmagnitude': '5.0', 'eventtype': 'earthquake', 'starttime': '2009-01-23 04:29:59', 'endtime': '2010-08-16 20:59:59'}\n",
      "Number of events to query: 2837 (of 5000 allowed)\n",
      "Number of events is below threshold, getting earthquake data ...\n",
      "Got data, parsing ...\n",
      "Number of events to parse: 2837\n",
      "\n",
      "Timeframe(s): ['2010-08-16 20:59:59', '2013-10-01 05:59:59', '2019-12-31 23:59:59']\n",
      "\n",
      "Query parameters: {'minlatitude': '-70.0', 'maxlatitude': '70.0', 'minlongitude': '100.0', 'maxlongitude': '300.0', 'format': 'geojson', 'minmagnitude': '5.0', 'eventtype': 'earthquake', 'starttime': '2010-08-16 20:59:59', 'endtime': '2013-10-01 05:59:59'}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of events to query: 5313 (of 5000 allowed)\n",
      "Number of events is too high, setting a reduced time frame!\n",
      "New timeframe(s): ['2010-08-16 20:59:59', '2012-03-09 13:29:59', '2013-10-01 05:59:59', '2019-12-31 23:59:59']\n",
      "\n",
      "Timeframe(s): ['2010-08-16 20:59:59', '2012-03-09 13:29:59', '2013-10-01 05:59:59', '2019-12-31 23:59:59']\n",
      "\n",
      "Query parameters: {'minlatitude': '-70.0', 'maxlatitude': '70.0', 'minlongitude': '100.0', 'maxlongitude': '300.0', 'format': 'geojson', 'minmagnitude': '5.0', 'eventtype': 'earthquake', 'starttime': '2010-08-16 20:59:59', 'endtime': '2012-03-09 13:29:59'}\n",
      "Number of events to query: 3363 (of 5000 allowed)\n",
      "Number of events is below threshold, getting earthquake data ...\n",
      "Got data, parsing ...\n",
      "Number of events to parse: 3363\n",
      "\n",
      "Timeframe(s): ['2012-03-09 13:29:59', '2013-10-01 05:59:59', '2019-12-31 23:59:59']\n",
      "\n",
      "Query parameters: {'minlatitude': '-70.0', 'maxlatitude': '70.0', 'minlongitude': '100.0', 'maxlongitude': '300.0', 'format': 'geojson', 'minmagnitude': '5.0', 'eventtype': 'earthquake', 'starttime': '2012-03-09 13:29:59', 'endtime': '2013-10-01 05:59:59'}\n",
      "Number of events to query: 1950 (of 5000 allowed)\n",
      "Number of events is below threshold, getting earthquake data ...\n",
      "Got data, parsing ...\n",
      "Number of events to parse: 1950\n",
      "\n",
      "Timeframe(s): ['2013-10-01 05:59:59', '2019-12-31 23:59:59']\n",
      "\n",
      "Query parameters: {'minlatitude': '-70.0', 'maxlatitude': '70.0', 'minlongitude': '100.0', 'maxlongitude': '300.0', 'format': 'geojson', 'minmagnitude': '5.0', 'eventtype': 'earthquake', 'starttime': '2013-10-01 05:59:59', 'endtime': '2019-12-31 23:59:59'}\n",
      "Number of events to query: 8121 (of 5000 allowed)\n",
      "Number of events is too high, setting a reduced time frame!\n",
      "New timeframe(s): ['2013-10-01 05:59:59', '2016-11-15 14:59:59', '2019-12-31 23:59:59']\n",
      "\n",
      "Timeframe(s): ['2013-10-01 05:59:59', '2016-11-15 14:59:59', '2019-12-31 23:59:59']\n",
      "\n",
      "Query parameters: {'minlatitude': '-70.0', 'maxlatitude': '70.0', 'minlongitude': '100.0', 'maxlongitude': '300.0', 'format': 'geojson', 'minmagnitude': '5.0', 'eventtype': 'earthquake', 'starttime': '2013-10-01 05:59:59', 'endtime': '2016-11-15 14:59:59'}\n",
      "Number of events to query: 3941 (of 5000 allowed)\n",
      "Number of events is below threshold, getting earthquake data ...\n",
      "Got data, parsing ...\n",
      "Number of events to parse: 3941\n",
      "\n",
      "Timeframe(s): ['2016-11-15 14:59:59', '2019-12-31 23:59:59']\n",
      "\n",
      "Query parameters: {'minlatitude': '-70.0', 'maxlatitude': '70.0', 'minlongitude': '100.0', 'maxlongitude': '300.0', 'format': 'geojson', 'minmagnitude': '5.0', 'eventtype': 'earthquake', 'starttime': '2016-11-15 14:59:59', 'endtime': '2019-12-31 23:59:59'}\n",
      "Number of events to query: 4180 (of 5000 allowed)\n",
      "Number of events is below threshold, getting earthquake data ...\n",
      "Got data, parsing ...\n",
      "Number of events to parse: 4180\n",
      "------------------------------------------------------------------------------------------------------\n",
      "\n",
      "Total number of parsed events in dataframe: 63878\n",
      "Runtime to query and parse the data: 300.3 s\n",
      "======================================================================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-01-10 13:03:11,183 - INFO - query_earthquakes: END main query method for earthquakes with 63878 earthquakes in 300.3 s.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>lon</th>\n",
       "      <th>lat</th>\n",
       "      <th>depth</th>\n",
       "      <th>mag</th>\n",
       "      <th>time</th>\n",
       "      <th>felt</th>\n",
       "      <th>cdi</th>\n",
       "      <th>mmi</th>\n",
       "      <th>alert</th>\n",
       "      <th>status</th>\n",
       "      <th>tsunami</th>\n",
       "      <th>nst</th>\n",
       "      <th>net</th>\n",
       "      <th>sig</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>usp00000hw</td>\n",
       "      <td>-101.0180</td>\n",
       "      <td>-36.1090</td>\n",
       "      <td>33.00</td>\n",
       "      <td>5.3</td>\n",
       "      <td>98610866900</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>reviewed</td>\n",
       "      <td>0</td>\n",
       "      <td>432</td>\n",
       "      <td>us</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>usp00000hv</td>\n",
       "      <td>-82.3720</td>\n",
       "      <td>5.9880</td>\n",
       "      <td>33.00</td>\n",
       "      <td>5.3</td>\n",
       "      <td>98610207400</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>reviewed</td>\n",
       "      <td>0</td>\n",
       "      <td>432</td>\n",
       "      <td>us</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>usp00000hs</td>\n",
       "      <td>103.0900</td>\n",
       "      <td>-4.3400</td>\n",
       "      <td>107.00</td>\n",
       "      <td>5.4</td>\n",
       "      <td>98602279500</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>reviewed</td>\n",
       "      <td>0</td>\n",
       "      <td>449</td>\n",
       "      <td>us</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>usp00000hn</td>\n",
       "      <td>-86.4730</td>\n",
       "      <td>11.5340</td>\n",
       "      <td>33.00</td>\n",
       "      <td>5.1</td>\n",
       "      <td>98583353700</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>reviewed</td>\n",
       "      <td>0</td>\n",
       "      <td>400</td>\n",
       "      <td>us</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>usp00000hk</td>\n",
       "      <td>141.5150</td>\n",
       "      <td>37.0400</td>\n",
       "      <td>56.00</td>\n",
       "      <td>5.1</td>\n",
       "      <td>98577374600</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>reviewed</td>\n",
       "      <td>0</td>\n",
       "      <td>400</td>\n",
       "      <td>us</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>63873</th>\n",
       "      <td>us10007a0b</td>\n",
       "      <td>173.2622</td>\n",
       "      <td>-42.5910</td>\n",
       "      <td>6.85</td>\n",
       "      <td>5.3</td>\n",
       "      <td>1479478977140</td>\n",
       "      <td>3</td>\n",
       "      <td>3.4</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>reviewed</td>\n",
       "      <td>0</td>\n",
       "      <td>433</td>\n",
       "      <td>us</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>63874</th>\n",
       "      <td>us100079g9</td>\n",
       "      <td>130.4786</td>\n",
       "      <td>-6.2582</td>\n",
       "      <td>112.15</td>\n",
       "      <td>5.5</td>\n",
       "      <td>1479401804290</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>3.25</td>\n",
       "      <td>green</td>\n",
       "      <td>reviewed</td>\n",
       "      <td>0</td>\n",
       "      <td>465</td>\n",
       "      <td>us</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>63875</th>\n",
       "      <td>us100078zy</td>\n",
       "      <td>-177.5663</td>\n",
       "      <td>-22.0950</td>\n",
       "      <td>296.01</td>\n",
       "      <td>5.3</td>\n",
       "      <td>1479327974490</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>2.38</td>\n",
       "      <td>green</td>\n",
       "      <td>reviewed</td>\n",
       "      <td>0</td>\n",
       "      <td>432</td>\n",
       "      <td>us</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>63876</th>\n",
       "      <td>us100078vh</td>\n",
       "      <td>113.2445</td>\n",
       "      <td>-9.0027</td>\n",
       "      <td>85.00</td>\n",
       "      <td>5.7</td>\n",
       "      <td>1479309011020</td>\n",
       "      <td>243</td>\n",
       "      <td>6.2</td>\n",
       "      <td>3.93</td>\n",
       "      <td>green</td>\n",
       "      <td>reviewed</td>\n",
       "      <td>0</td>\n",
       "      <td>651</td>\n",
       "      <td>us</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>63877</th>\n",
       "      <td>us100078ll</td>\n",
       "      <td>-173.0161</td>\n",
       "      <td>-20.2391</td>\n",
       "      <td>20.00</td>\n",
       "      <td>5.8</td>\n",
       "      <td>1479256229860</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>3.17</td>\n",
       "      <td>green</td>\n",
       "      <td>reviewed</td>\n",
       "      <td>0</td>\n",
       "      <td>518</td>\n",
       "      <td>us</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>63878 rows Ã— 15 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "               id       lon      lat   depth  mag           time  felt   cdi  \\\n",
       "0      usp00000hw -101.0180 -36.1090   33.00  5.3    98610866900  None  None   \n",
       "1      usp00000hv  -82.3720   5.9880   33.00  5.3    98610207400  None  None   \n",
       "2      usp00000hs  103.0900  -4.3400  107.00  5.4    98602279500  None  None   \n",
       "3      usp00000hn  -86.4730  11.5340   33.00  5.1    98583353700  None  None   \n",
       "4      usp00000hk  141.5150  37.0400   56.00  5.1    98577374600  None  None   \n",
       "...           ...       ...      ...     ...  ...            ...   ...   ...   \n",
       "63873  us10007a0b  173.2622 -42.5910    6.85  5.3  1479478977140     3   3.4   \n",
       "63874  us100079g9  130.4786  -6.2582  112.15  5.5  1479401804290  None  None   \n",
       "63875  us100078zy -177.5663 -22.0950  296.01  5.3  1479327974490  None  None   \n",
       "63876  us100078vh  113.2445  -9.0027   85.00  5.7  1479309011020   243   6.2   \n",
       "63877  us100078ll -173.0161 -20.2391   20.00  5.8  1479256229860  None  None   \n",
       "\n",
       "        mmi  alert    status  tsunami  nst net   sig  \n",
       "0      None   None  reviewed        0  432  us  None  \n",
       "1      None   None  reviewed        0  432  us  None  \n",
       "2      None   None  reviewed        0  449  us  None  \n",
       "3      None   None  reviewed        0  400  us  None  \n",
       "4      None   None  reviewed        0  400  us  None  \n",
       "...     ...    ...       ...      ...  ...  ..   ...  \n",
       "63873  None   None  reviewed        0  433  us  None  \n",
       "63874  3.25  green  reviewed        0  465  us  None  \n",
       "63875  2.38  green  reviewed        0  432  us  None  \n",
       "63876  3.93  green  reviewed        0  651  us  None  \n",
       "63877  3.17  green  reviewed        0  518  us  None  \n",
       "\n",
       "[63878 rows x 15 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# set the dynamic query timeframe (starttime and endtime in datetime format) and parameters\n",
    "query_timeframe = [\"1970-01-01 00:00:00\", \"2019-12-31 23:59:59\"]\n",
    "api_query_parameters = {\n",
    "    \"minlatitude\": '-70.0', \"maxlatitude\": '70.0', \n",
    "    \"minlongitude\": '100.0', \"maxlongitude\": '300.0',\n",
    "    \"format\": \"geojson\",\n",
    "    \"minmagnitude\": \"5.0\",\n",
    "    \"eventtype\": \"earthquake\"\n",
    "}\n",
    "\n",
    "# get data for this area, add area information\n",
    "earthquakes = query_earthquakes(query_timeframe = query_timeframe, \n",
    "                                api_query_parameters = api_query_parameters, \n",
    "                                verbosity = 0)\n",
    "\n",
    "# show earthquakes\n",
    "display(earthquakes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Save scraped earthquake dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-01-10 13:03:48,572 - INFO - save_dataset: Data saved successfully to 'data/earthquakes_scraped_250110-130348.csv'.\n"
     ]
    }
   ],
   "source": [
    "# save earthquake dataset\n",
    "save_dataset(data_file = \"earthquakes_scraped.csv\", \n",
    "             data_dir = data_dir, \n",
    "             data_set = earthquakes\n",
    "            )  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### End of script"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-01-10 13:03:51,266 - INFO - End of script '10_scrape_earthquake_data.ipynb'.\n"
     ]
    }
   ],
   "source": [
    "# log the end of this script\n",
    "logging.info(f\"End of script '{script_name}'.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
